#include <gtest/gtest.h>
#include <cuda_runtime.h>
#include <fstream>  // for std::ifstream
#include <cstdio>   // for std::remove
#include <cmath>    // for std::isnan, std::isinf
#include "../include/ltc_block.h"
#include "../include/cutensor_ops.h"

namespace cudatrader {
namespace {

class LTCBlockTest : public ::testing::Test {
protected:
    void SetUp() override {
        // Initialize cuTENSOR
        cutensor_ops::initialize();
    }
    
    void TearDown() override {
        // Clean up cuTENSOR
        cutensor_ops::cleanup();
    }
    
    // Helper function to create a random input tensor
    CudaMemory<float> createRandomTensor(size_t batch_size, size_t seq_len, size_t feature_dim) {
        // Create host memory with random values
        std::vector<float> host_data(batch_size * seq_len * feature_dim);
        for (size_t i = 0; i < host_data.size(); ++i) {
            host_data[i] = static_cast<float>(std::rand()) / RAND_MAX * 2.0f - 1.0f;
        }
        
        // Create device memory and copy data
        CudaMemory<float> device_data(batch_size * seq_len * feature_dim);
        cudaMemcpy(device_data.get(), host_data.data(), host_data.size() * sizeof(float), cudaMemcpyHostToDevice);
        
        return device_data;
    }
    
    // Helper function to check for NaN/Inf values
    bool checkForNanInf(const CudaMemory<float>& tensor) {
        std::vector<float> host_data(tensor.size());
        cudaMemcpy(host_data.data(), tensor.get(), tensor.size() * sizeof(float), cudaMemcpyDeviceToHost);
        
        for (size_t i = 0; i < tensor.size(); ++i) {
            if (std::isnan(host_data[i]) || std::isinf(host_data[i])) {
                return true;
            }
        }
        return false;
    }
    
    void compareTensors(const CudaMemory<float>& tensor1, const CudaMemory<float>& tensor2, float tolerance) {
        ASSERT_EQ(tensor1.size(), tensor2.size()) << "Tensor sizes don't match";
        
        // Copy tensors to host
        std::vector<float> host_tensor1(tensor1.size());
        std::vector<float> host_tensor2(tensor2.size());
        cudaMemcpy(host_tensor1.data(), tensor1.get(), tensor1.size() * sizeof(float), cudaMemcpyDeviceToHost);
        cudaMemcpy(host_tensor2.data(), tensor2.get(), tensor2.size() * sizeof(float), cudaMemcpyDeviceToHost);
        
        // Compare tensors with better error reporting
        int nan_count = 0;
        int mismatch_count = 0;
        float max_diff = 0.0f;
        
        for (size_t i = 0; i < tensor1.size(); ++i) {
            float val1 = host_tensor1[i];
            float val2 = host_tensor2[i];
            
            // Check for NaN values
            if (std::isnan(val1) || std::isnan(val2)) {
                nan_count++;
                continue;
            }
            
            float diff = std::abs(val1 - val2);
            max_diff = std::max(max_diff, diff);
            
            if (diff > tolerance) {
                mismatch_count++;
            }
        }
        
        // Report results
        if (nan_count > 0) {
            std::cout << "Warning: Found " << nan_count << " NaN values in tensors." << std::endl;
        }
        
        // Use a more reasonable tolerance threshold for FP32
        const float allowed_mismatch_percentage = 0.05f; // 5%
        const int allowed_mismatches = static_cast<int>(tensor1.size() * allowed_mismatch_percentage);
        
        EXPECT_LE(mismatch_count, allowed_mismatches) 
            << "Found " << mismatch_count << " values exceeding tolerance out of " << tensor1.size() 
            << " (allowed: " << allowed_mismatches << ")";
        
        if (mismatch_count > 0) {
            std::cout << "Maximum difference: " << max_diff << " (tolerance: " << tolerance << ")" << std::endl;
        }
    }
};

TEST_F(LTCBlockTest, ConstructorTest) {
    // Test that the constructor works without errors
    ASSERT_NO_THROW({
        LTCBlock block(64, 128);
    });
    
    // Test with tensor core optimized dimensions and multiple layers
    ASSERT_NO_THROW({
        LTCBlock block(64, 128, 3, LTCPoolingMethod::MEAN);
    });
    
    // Test with non-tensor core optimized dimensions
    ASSERT_NO_THROW({
        LTCBlock block(65, 127, 2, LTCPoolingMethod::LAST);
    });
    
    // Test with attention pooling
    ASSERT_NO_THROW({
        LTCBlock block(64, 128, 2, LTCPoolingMethod::ATTENTION);
    });
    
    // Test with explicit FP32 precision and Fused ODE integration
    ASSERT_NO_THROW({
        LTCBlock block(64, 128, 2, LTCPoolingMethod::MEAN, 0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    });
    
    // Test with explicit FP16 precision and Fused ODE integration
    ASSERT_NO_THROW({
        LTCBlock block(64, 128, 2, LTCPoolingMethod::MEAN, 0.05f, 0.5f, 1e-3f, true, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    });
}

TEST_F(LTCBlockTest, TensorCoreOptimizationTest) {
    // Test with tensor core optimized dimensions
    LTCBlock block_optimized(64, 128);
    EXPECT_TRUE(block_optimized.isTensorCoreOptimized());
    
    // Test with non-tensor core optimized dimensions
    LTCBlock block_non_optimized(65, 127);
    EXPECT_FALSE(block_non_optimized.isTensorCoreOptimized());
}

TEST_F(LTCBlockTest, ForwardPassMeanPoolingTest) {
    // Create LTC block with small dimensions for testing
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    
    LTCBlock block(input_dim, hidden_dim, 1, LTCPoolingMethod::MEAN);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass with explicit batch_size and seq_len
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Verify output shape (should be [batch_size, hidden_dim])
    EXPECT_EQ(output.size(), batch_size * hidden_dim);
}

TEST_F(LTCBlockTest, ForwardPassLastPoolingTest) {
    // Create LTC block with small dimensions for testing
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    
    LTCBlock block(input_dim, hidden_dim, 1, LTCPoolingMethod::LAST);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass with explicit batch_size and seq_len
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Verify output shape (should be [batch_size, hidden_dim])
    EXPECT_EQ(output.size(), batch_size * hidden_dim);
}

TEST_F(LTCBlockTest, ForwardPassAttentionPoolingTest) {
    // Create LTC block with small dimensions for testing
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    
    LTCBlock block(input_dim, hidden_dim, 1, LTCPoolingMethod::ATTENTION);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass with explicit batch_size and seq_len
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Verify output shape (should be [batch_size, hidden_dim])
    EXPECT_EQ(output.size(), batch_size * hidden_dim);
}

TEST_F(LTCBlockTest, MultiLayerTest) {
    // Create LTC block with multiple layers
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    const int num_layers = 3;
    
    LTCBlock block(input_dim, hidden_dim, num_layers, LTCPoolingMethod::MEAN);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass with explicit batch_size and seq_len
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Verify output shape (should be [batch_size, hidden_dim])
    EXPECT_EQ(output.size(), batch_size * hidden_dim);
    
    // We don't have a getNumLayers() method, so we'll just check that the block was created successfully
    // and the forward pass works as expected
}

TEST_F(LTCBlockTest, TauRegularizerTest) {
    // Create LTC block
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int num_layers = 2;
    const float tau_reg_strength = 0.05f;
    
    LTCBlock block(input_dim, hidden_dim, num_layers, LTCPoolingMethod::LAST, 
                  0.05f, 0.5f, 1e-3f, false, tau_reg_strength, LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Get tau regularizer value without applying strength
    float tau_reg_raw = block.tauRegularizer(false);
    
    // Get tau regularizer value with strength applied
    float tau_reg_with_strength = block.tauRegularizer(true);
    
    // Verify that applying strength scales the regularization value correctly
    EXPECT_FLOAT_EQ(tau_reg_with_strength, tau_reg_raw * tau_reg_strength);
}

TEST_F(LTCBlockTest, PrecisionAndIntegrationMethodTest) {
    // Create LTC blocks with different precision and integration methods
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    
    // FP32 with Fused ODE (recommended for RTX 5070)
    LTCBlock block_fp32_fused(input_dim, hidden_dim, 1, LTCPoolingMethod::MEAN,
                             0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // FP32 with Fused ODE
    LTCBlock block_fp32_euler(input_dim, hidden_dim, 1, LTCPoolingMethod::MEAN,
                             0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass with both blocks
    auto output_fp32_fused = block_fp32_fused.forward(x_seq, batch_size, seq_len);
    auto output_fp32_euler = block_fp32_euler.forward(x_seq, batch_size, seq_len);
    
    // Compare outputs - they should be similar but not identical due to different integration methods
    compareTensors(output_fp32_fused, output_fp32_euler, 0.1f);
    
    // Test throughput comparison between Fused ODE and Fused ODE integration methods
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    
    const int num_iterations = 10;
    
    // Measure Fused ODE throughput
    cudaEventRecord(start);
    for (int i = 0; i < num_iterations; ++i) {
        auto output = block_fp32_fused.forward(x_seq, batch_size, seq_len);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float fused_ms = 0.0f;
    cudaEventElapsedTime(&fused_ms, start, stop);
    float fused_steps_per_sec = num_iterations / (fused_ms * 1e-3);
    
    // Measure Fused ODE throughput
    cudaEventRecord(start);
    for (int i = 0; i < num_iterations; ++i) {
        auto output = block_fp32_euler.forward(x_seq, batch_size, seq_len);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float euler_ms = 0.0f;
    cudaEventElapsedTime(&euler_ms, start, stop);
    float euler_steps_per_sec = num_iterations / (euler_ms * 1e-3);
    
    // Calculate speedup
    float speedup = fused_steps_per_sec / euler_steps_per_sec;
    
    std::cout << "Fused ODE throughput: " << fused_steps_per_sec << " steps/sec\n"
              << "Fused ODE throughput: " << euler_steps_per_sec << " steps/sec\n"
              << "Fused/Fused throughput ratio: " << speedup << "x\n";
    
    // Clean up
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    
    // Note: We're not asserting specific performance numbers as they depend on hardware
    std::cout << "Note: On RTX 5070, FP32 with Fused ODE integration method provides the best\n"
              << "performance and numerical stability based on our empirical findings.\n";
}

TEST_F(LTCBlockTest, WeightInitializationTest) {
    // Create LTC block
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int num_layers = 2;
    
    // Use FP32 precision with Fused ODE integration method
    LTCBlock block(input_dim, hidden_dim, num_layers, LTCPoolingMethod::LAST,
                  0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Re-initialize weights
    ASSERT_NO_THROW({
        block.initializeWeights();
    });
}

TEST_F(LTCBlockTest, WeightSaveLoadTest) {
    // Create LTC block
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int num_layers = 2;
    
    // Use FP32 precision with Fused ODE integration method
    LTCBlock block(input_dim, hidden_dim, num_layers, LTCPoolingMethod::LAST,
                  0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Explicitly initialize weights to ensure they're not NaN
    ASSERT_NO_THROW({
        block.initializeWeights();
    });
    
    // Save weights
    const std::string weights_path = "/tmp/ltc_block_weights.bin";
    ASSERT_NO_THROW({
        block.saveWeights(weights_path);
    });
    
    // Create a new block and load weights
    LTCBlock block2(input_dim, hidden_dim, num_layers, LTCPoolingMethod::LAST,
                   0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
    ASSERT_NO_THROW({
        block2.loadWeights(weights_path);
    });
    
    // Compare weights between the original and loaded blocks by testing with identical inputs
    const int batch_size = 2;
    const int seq_len = 8;
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass on both blocks with the same input
    cudaStream_t stream = nullptr;
    auto output_original = block.forward(x_seq, batch_size, seq_len, stream);
    auto output_loaded = block2.forward(x_seq, batch_size, seq_len, stream);
    
    // Synchronize to ensure all operations are complete
    cudaDeviceSynchronize();
    
    // The outputs should be identical if weights were loaded correctly
    // Use a lower tolerance for FP32 operations
    compareTensors(output_loaded, output_original, 0.03f);
    
    // Test with different batch size and sequence length to ensure robustness
    const int batch_size2 = 3;
    const int seq_len2 = 10;
    
    auto x_seq2 = createRandomTensor(batch_size2, seq_len2, input_dim);
    
    auto output_original2 = block.forward(x_seq2, batch_size2, seq_len2, stream);
    auto output_loaded2 = block2.forward(x_seq2, batch_size2, seq_len2, stream);
    
    // Synchronize again
    cudaDeviceSynchronize();
    
    // Compare with lower tolerance for FP32 operations
    compareTensors(output_loaded2, output_original2, 0.03f);
}

TEST_F(LTCBlockTest, ThroughputTest) {
    // Test throughput with different batch sizes and sequence lengths
    std::cout << "\n=== LTC Block Throughput Test (FP32) ===\n";
    std::cout << "Batch\tSeqLen\tMean GB/s\tLast GB/s\tAttn GB/s\n";
    
    const int input_dim = 32;
    const int hidden_dim = 64;
    const int num_iterations = 10;
    
    // Test different batch sizes and sequence lengths
    std::vector<int> batch_sizes = {1, 2, 4, 6};
    std::vector<int> seq_lengths = {4, 8, 16};
    
    for (int batch_size : batch_sizes) {
        for (int seq_len : seq_lengths) {
            try {
                // Create LTC block with FP32 precision and Fused ODE integration
                LTCBlock block_mean(input_dim, hidden_dim, 1, LTCPoolingMethod::MEAN,
                                   0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
                
                LTCBlock block_last(input_dim, hidden_dim, 1, LTCPoolingMethod::LAST,
                                   0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
                
                LTCBlock block_attention(input_dim, hidden_dim, 1, LTCPoolingMethod::ATTENTION,
                                        0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
                
                // Create random input sequence
                auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
                
                // Create events for timing
                cudaEvent_t start, stop;
                cudaEventCreate(&start);
                cudaEventCreate(&stop);
                
                // Test mean pooling throughput
                cudaEventRecord(start);
                for (int i = 0; i < num_iterations; ++i) {
                    auto output = block_mean.forward(x_seq, batch_size, seq_len);
                    cudaDeviceSynchronize();  // Ensure memory is released
                }
                cudaEventRecord(stop);
                cudaEventSynchronize(stop);
                
                float mean_ms = 0.0f;
                cudaEventElapsedTime(&mean_ms, start, stop);
                float mean_gb_per_s = (batch_size * seq_len * input_dim * sizeof(float) * num_iterations) / (mean_ms * 1e-3) / 1e9;
                
                // Test last state pooling throughput
                cudaEventRecord(start);
                for (int i = 0; i < num_iterations; ++i) {
                    auto output = block_last.forward(x_seq, batch_size, seq_len);
                    cudaDeviceSynchronize();  // Ensure memory is released
                }
                cudaEventRecord(stop);
                cudaEventSynchronize(stop);
                
                float last_ms = 0.0f;
                cudaEventElapsedTime(&last_ms, start, stop);
                float last_gb_per_s = (batch_size * seq_len * input_dim * sizeof(float) * num_iterations) / (last_ms * 1e-3) / 1e9;
                
                // Test attention pooling throughput
                cudaEventRecord(start);
                for (int i = 0; i < num_iterations; ++i) {
                    auto output = block_attention.forward(x_seq, batch_size, seq_len);
                    cudaDeviceSynchronize();  // Ensure memory is released
                }
                cudaEventRecord(stop);
                cudaEventSynchronize(stop);
                
                float attention_ms = 0.0f;
                cudaEventElapsedTime(&attention_ms, start, stop);
                float attention_gb_per_s = (batch_size * seq_len * input_dim * sizeof(float) * num_iterations) / (attention_ms * 1e-3) / 1e9;
                
                std::cout << batch_size << "\t" << seq_len << "\t" 
                          << mean_gb_per_s << "\t" 
                          << last_gb_per_s << "\t" 
                          << attention_gb_per_s << std::endl;
                
                // Clean up events
                cudaEventDestroy(start);
                cudaEventDestroy(stop);
            } catch (const std::exception& e) {
                std::cerr << "Exception at batch_size=" << batch_size << ", seq_len=" << seq_len 
                          << ": " << e.what() << std::endl;
                // Continue with the next configuration
                continue;
            }
        }
    }
}

TEST_F(LTCBlockTest, MultiLayerThroughputTest) {
    // Test throughput with different numbers of layers
    std::cout << "\n=== LTC Block Multi-Layer Throughput Test (FP32) ===\n";
    std::cout << "Layers\tGB/s\n";
    
    const int input_dim = 32;
    const int hidden_dim = 64;
    const int batch_size = 4;
    const int seq_len = 20;
    const int num_iterations = 10;
    
    // Test different numbers of layers
    std::vector<int> layer_counts = {1, 2, 3, 4, 5};
    
    for (int num_layers : layer_counts) {
        try {
            // Create LTC block with FP32 precision and Fused ODE integration
            LTCBlock block(input_dim, hidden_dim, num_layers, LTCPoolingMethod::MEAN,
                          0.05f, 0.5f, 1e-3f, false, 0.01f, LTCIntegrationMethod::FUSED_ODE_FP32);
            
            // Create random input sequence using the existing helper method
            auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
            
            // Create events for timing
            cudaEvent_t start, stop;
            cudaEventCreate(&start);
            cudaEventCreate(&stop);
            
            // Test multi-layer throughput
            cudaEventRecord(start);
            for (int i = 0; i < num_iterations; ++i) {
                auto output = block.forward(x_seq, batch_size, seq_len);
                cudaDeviceSynchronize();  // Ensure memory is released
            }
            cudaEventRecord(stop);
            cudaEventSynchronize(stop);
            
            float multi_layer_ms = 0.0f;
            cudaEventElapsedTime(&multi_layer_ms, start, stop);
            
            // For multi-layer, we process the input once and then process the hidden state for each additional layer
            float bytes_processed = (batch_size * seq_len * input_dim + (num_layers - 1) * batch_size * seq_len * hidden_dim) * sizeof(float) * num_iterations;
            float multi_layer_gb_per_s = bytes_processed / (multi_layer_ms * 1e-3) / 1e9;
            
            std::cout << num_layers << "\t" << multi_layer_gb_per_s << std::endl;
            
            // Clean up events
            cudaEventDestroy(start);
            cudaEventDestroy(stop);
        } catch (const std::exception& e) {
            std::cerr << "Exception at num_layers=" << num_layers << ": " << e.what() << std::endl;
            // Continue with the next configuration
            continue;
        }
    }
}

TEST_F(LTCBlockTest, RegularizationAndIntegrationMethodTest) {
    // Create LTC block with multiple layers
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int num_layers = 3;
    const float initial_reg_strength = 0.01f;
    const float new_reg_strength = 0.05f;
    
    // Create block with FP32 precision and Fused ODE integration
    LTCBlock block(input_dim, hidden_dim, num_layers, LTCPoolingMethod::MEAN,
                  0.05f, 0.5f, 1e-3f, false, initial_reg_strength, LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Verify initial regularization strength
    EXPECT_FLOAT_EQ(block.getTauRegularizationStrength(), initial_reg_strength);
    EXPECT_EQ(block.getIntegrationMethod(), LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Get initial regularization value
    float initial_reg = block.tauRegularizer(true);
    float initial_reg_raw = block.tauRegularizer(false);
    
    // Verify that applying strength scales the regularization value correctly
    EXPECT_FLOAT_EQ(initial_reg, initial_reg_raw * initial_reg_strength);
    
    // Change regularization strength
    block.setTauRegularizationStrength(new_reg_strength);
    
    // Verify new regularization strength
    EXPECT_FLOAT_EQ(block.getTauRegularizationStrength(), new_reg_strength);
    
    // Get new regularization value
    float new_reg = block.tauRegularizer(true);
    
    // Verify that the new regularization value is scaled by the new strength
    EXPECT_FLOAT_EQ(new_reg, initial_reg_raw * new_reg_strength);
    
    // Change integration method to Fused ODE
    block.setIntegrationMethod(LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Verify new integration method
    EXPECT_EQ(block.getIntegrationMethod(), LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Change back to Fused ODE
    block.setIntegrationMethod(LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Verify integration method changed back
    EXPECT_EQ(block.getIntegrationMethod(), LTCIntegrationMethod::FUSED_ODE_FP32);
    
    // Test with invalid regularization strength
    EXPECT_THROW(block.setTauRegularizationStrength(-0.1f), std::invalid_argument);
}

TEST_F(LTCBlockTest, BackwardPassMeanPoolingTest) {
    // Create LTC block with small dimensions for testing
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    const int num_layers = 2;
    
    // Test backward pass with MEAN pooling
    LTCPoolingMethod pooling_method = LTCPoolingMethod::MEAN;
    std::cout << "Testing backward pass with MEAN pooling" << std::endl;
    
    // Create block
    LTCBlock block(input_dim, hidden_dim, num_layers, pooling_method);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Create gradient for output (simulating loss gradient)
    CudaMemory<float> grad_output(batch_size * hidden_dim);
    std::vector<float> host_grad(batch_size * hidden_dim, 1.0f);
    cudaMemcpy(grad_output.get(), host_grad.data(), 
              host_grad.size() * sizeof(float), cudaMemcpyHostToDevice);
    
    // Run backward pass
    ASSERT_NO_THROW({
        auto gradients = block.backward(grad_output, x_seq, batch_size, seq_len);
        
        // Verify gradient shapes
        EXPECT_EQ(gradients.grad_x_seq.size(), batch_size * seq_len * input_dim);
        EXPECT_EQ(gradients.cell_gradients.size(), num_layers);
    });
}

TEST_F(LTCBlockTest, BackwardPassLastPoolingTest) {
    // Create LTC block with small dimensions for testing
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    const int num_layers = 2;
    
    // Test backward pass with LAST pooling
    LTCPoolingMethod pooling_method = LTCPoolingMethod::LAST;
    std::cout << "Testing backward pass with LAST pooling" << std::endl;
    
    // Create block
    LTCBlock block(input_dim, hidden_dim, num_layers, pooling_method);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Create gradient for output (simulating loss gradient)
    CudaMemory<float> grad_output(batch_size * hidden_dim);
    std::vector<float> host_grad(batch_size * hidden_dim, 1.0f);
    cudaMemcpy(grad_output.get(), host_grad.data(), 
              host_grad.size() * sizeof(float), cudaMemcpyHostToDevice);
    
    // Run backward pass
    ASSERT_NO_THROW({
        auto gradients = block.backward(grad_output, x_seq, batch_size, seq_len);
        
        // Verify gradient shapes
        EXPECT_EQ(gradients.grad_x_seq.size(), batch_size * seq_len * input_dim);
        EXPECT_EQ(gradients.cell_gradients.size(), num_layers);
    });
}

TEST_F(LTCBlockTest, BackwardPassAttentionPoolingTest) {
    // Create LTC block with small dimensions for testing
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    const int num_layers = 2;
    
    // Test backward pass with ATTENTION pooling
    LTCPoolingMethod pooling_method = LTCPoolingMethod::ATTENTION;
    std::cout << "Testing backward pass with ATTENTION pooling" << std::endl;
    
    // Create block
    LTCBlock block(input_dim, hidden_dim, num_layers, pooling_method);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Create gradient for output (simulating loss gradient)
    CudaMemory<float> grad_output(batch_size * hidden_dim);
    std::vector<float> host_grad(batch_size * hidden_dim, 1.0f);
    cudaMemcpy(grad_output.get(), host_grad.data(), 
              host_grad.size() * sizeof(float), cudaMemcpyHostToDevice);
    
    // Run backward pass
    ASSERT_NO_THROW({
        auto gradients = block.backward(grad_output, x_seq, batch_size, seq_len);
        
        // Verify gradient shapes
        EXPECT_EQ(gradients.grad_x_seq.size(), batch_size * seq_len * input_dim);
        EXPECT_EQ(gradients.grad_attention_vector.size(), hidden_dim);
        EXPECT_EQ(gradients.cell_gradients.size(), num_layers);
    });
}

TEST_F(LTCBlockTest, BackwardPassAndWeightUpdateTest) {
    // Create LTC block with small dimensions
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    const float learning_rate = 0.01f;
    
    // Test with attention pooling to also test attention vector update
    LTCBlock block(input_dim, hidden_dim, 2, LTCPoolingMethod::ATTENTION);
    
    // Create random input sequence
    auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
    
    // Run forward pass
    auto output = block.forward(x_seq, batch_size, seq_len);
    
    // Create gradient for output
    CudaMemory<float> grad_output(batch_size * hidden_dim);
    std::vector<float> host_grad(batch_size * hidden_dim, 0.1f);
    cudaMemcpy(grad_output.get(), host_grad.data(), 
              host_grad.size() * sizeof(float), cudaMemcpyHostToDevice);
    
    // Run backward pass
    auto gradients = block.backward(grad_output, x_seq, batch_size, seq_len);
    
    // Verify gradients were computed
    ASSERT_EQ(gradients.cell_gradients.size(), 2);  // 2 layers
    
    // Verify gradient dimensions
    EXPECT_EQ(gradients.grad_x_seq.size(), batch_size * seq_len * input_dim);
    
    // For attention pooling, verify attention gradient dimensions
    if (block.getPoolingMethod() == LTCPoolingMethod::ATTENTION) {
        EXPECT_EQ(gradients.grad_attention_vector.size(), hidden_dim);
    }
    
    // Update weights - this should not throw
    ASSERT_NO_THROW({
        block.updateWeights(gradients, learning_rate);
    });
    
    // For attention pooling, run forward pass again to verify weights were updated
    if (block.getPoolingMethod() == LTCPoolingMethod::ATTENTION) {
        auto output2 = block.forward(x_seq, batch_size, seq_len);
        
        // Outputs should be different after weight update
        std::vector<float> host_output1(output.size());
        std::vector<float> host_output2(output2.size());
        
        cudaMemcpy(host_output1.data(), output.get(), 
                  output.size() * sizeof(float), cudaMemcpyDeviceToHost);
        cudaMemcpy(host_output2.data(), output2.get(), 
                  output2.size() * sizeof(float), cudaMemcpyDeviceToHost);
        
        bool outputs_differ = false;
        for (size_t i = 0; i < host_output1.size(); ++i) {
            if (std::abs(host_output1[i] - host_output2[i]) > 1e-6f) {
                outputs_differ = true;
                break;
            }
        }
        
        EXPECT_TRUE(outputs_differ) << "Outputs should differ after attention weight update";
    }
}

TEST_F(LTCBlockTest, BackwardPassPoolingMethodsTest) {
    const int input_dim = 16;
    const int hidden_dim = 32;
    const int batch_size = 2;
    const int seq_len = 5;
    const int num_layers = 2;
    
    std::vector<LTCPoolingMethod> pooling_methods = {
        LTCPoolingMethod::MEAN,
        LTCPoolingMethod::LAST,
        LTCPoolingMethod::ATTENTION
    };
    
    for (auto pooling_method : pooling_methods) {
        LTCBlock block(input_dim, hidden_dim, num_layers, pooling_method);
        
        // Create random input
        auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
        
        // Forward pass
        auto output = block.forward(x_seq, batch_size, seq_len);
        
        // Create gradient matching output size
        auto grad_output = createRandomTensor(batch_size, 1, hidden_dim);
        
        // Backward pass
        ASSERT_NO_THROW({
            auto gradients = block.backward(grad_output, x_seq, batch_size, seq_len);
            
            // Verify gradient structure
            EXPECT_EQ(gradients.cell_gradients.size(), num_layers);
            EXPECT_EQ(gradients.grad_x_seq.size(), batch_size * seq_len * input_dim);
            
            if (pooling_method == LTCPoolingMethod::ATTENTION) {
                EXPECT_EQ(gradients.grad_attention_vector.size(), hidden_dim);
            }
        });
    }
}

TEST_F(LTCBlockTest, UpdateWeightsTest) {
    const int input_dim = 32;
    const int hidden_dim = 64;
    const int batch_size = 4;
    const int seq_len = 5;
    const int num_layers = 2;
    const float learning_rate = 0.01f;
    
    // Test with different pooling methods
    std::vector<LTCPoolingMethod> pooling_methods = {
        LTCPoolingMethod::MEAN,
        LTCPoolingMethod::LAST,
        LTCPoolingMethod::ATTENTION
    };
    
    for (auto pooling_method : pooling_methods) {
        std::cout << "\nTesting updateWeights with pooling method: ";
        switch (pooling_method) {
            case LTCPoolingMethod::MEAN: std::cout << "MEAN"; break;
            case LTCPoolingMethod::LAST: std::cout << "LAST"; break;
            case LTCPoolingMethod::ATTENTION: std::cout << "ATTENTION"; break;
        }
        std::cout << std::endl;
        
        try {
            // Create LTC block
            LTCBlock block(input_dim, hidden_dim, num_layers, pooling_method);
            
            // Create random input
            auto x_seq = createRandomTensor(batch_size, seq_len, input_dim);
            
            // Forward pass
            auto output_before = block.forward(x_seq, batch_size, seq_len);
            
            // Save weights before update
            std::string weights_file_before = "/tmp/ltc_block_weights_before.bin";
            block.saveWeights(weights_file_before);
            
            // Create gradient for backward pass
            auto grad_output = createRandomTensor(batch_size, 1, hidden_dim);
            
            // Backward pass to get gradients
            auto gradients = block.backward(grad_output, x_seq, batch_size, seq_len);
            
            // Apply weight update
            block.updateWeights(gradients, learning_rate);
            cudaDeviceSynchronize();
            
            // Save weights after update
            std::string weights_file_after = "/tmp/ltc_block_weights_after.bin";
            block.saveWeights(weights_file_after);
            
            // Load and compare file sizes
            auto getFileSize = [](const std::string& filename) -> size_t {
                std::ifstream file(filename, std::ios::binary | std::ios::ate);
                if (!file.is_open()) {
                    throw std::runtime_error("Failed to open file: " + filename);
                }
                return file.tellg();
            };
            
            size_t size_before = getFileSize(weights_file_before);
            size_t size_after = getFileSize(weights_file_after);
            
            ASSERT_EQ(size_before, size_after) << "Weight file sizes should match";
            
            // Perform forward pass after weight update
            auto output_after = block.forward(x_seq, batch_size, seq_len);
            
            // Check for NaN/Inf after update
            ASSERT_FALSE(checkForNanInf(output_after)) << "NaN/Inf detected after weight update";
            
            // Check that outputs have changed
            std::vector<float> output_before_host(output_before.size());
            std::vector<float> output_after_host(output_after.size());
            cudaMemcpy(output_before_host.data(), output_before.get(), 
                      output_before.size() * sizeof(float), cudaMemcpyDeviceToHost);
            cudaMemcpy(output_after_host.data(), output_after.get(), 
                      output_after.size() * sizeof(float), cudaMemcpyDeviceToHost);
            
            bool outputs_changed = false;
            float max_diff = 0.0f;
            for (size_t i = 0; i < output_before_host.size(); ++i) {
                float diff = std::abs(output_after_host[i] - output_before_host[i]);
                if (diff > 1e-6f) {
                    outputs_changed = true;
                }
                max_diff = std::max(max_diff, diff);
            }
            
            EXPECT_TRUE(outputs_changed) << "Outputs should change after weight update";
            EXPECT_GT(max_diff, 0.0f) << "Maximum output difference should be positive";
            
            std::cout << "âœ… UpdateWeights test passed for " 
                      << (pooling_method == LTCPoolingMethod::MEAN ? "MEAN" :
                          pooling_method == LTCPoolingMethod::LAST ? "LAST" : "ATTENTION")
                      << " pooling\n"
                      << "  - Maximum output difference: " << max_diff << "\n";
            
            // Clean up temporary files
            std::remove(weights_file_before.c_str());
            std::remove(weights_file_after.c_str());
            
        } catch (const std::exception& e) {
            FAIL() << "UpdateWeights test failed for pooling method with exception: " << e.what();
        }
    }
}

} // namespace
} // namespace cudatrader

int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}
